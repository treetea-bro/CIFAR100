{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws3nFTNOv6cb",
        "outputId": "0f306c9b-89e5-4684-f865-224de72a94fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=136ff5f2172f20872548e597ef333b83dc42d6e0e79971a019aa2c4816f71814\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IRJ-UNkUvhcI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch.functional import Tensor\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import fnmatch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 변수들이 담긴 dictionary\n",
        "args = {\n",
        "    \"lr\": 0.01,\n",
        "    \"gamma\": 0.1,\n",
        "    \"wd\": 1e-6,\n",
        "    \"ne\": 30,\n",
        "    \"nsc\": 10,\n",
        "    \"batch_split\": 1,\n",
        "    \"batch\": 32,\n",
        "    \"alpha\": 0.1,\n",
        "    \"model\": \"efficientnet-b7\",\n",
        "    \"checkpoint_save_directory\": \"./checkpoint\",\n",
        "    \"checkpoint_threshold\": 90.0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "7hUhSsWPvtZg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시드 설정으로 다음에도 같은 accuracy 결과가 나올 수 있도록 함\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# 현재PC에서 GPU 이용가능 여부 확인해서 변수에 이용가능한 GPU 종류담기\n",
        "# apple silicon이면 mps를 사용하고, 아니면 cuda를 이용하고, cuda도 안되면 cpu 이용\n",
        "device = (\n",
        "    \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ")\n",
        "print(\"device : \", device)\n",
        "\n",
        "# 데이터로더에서 사용할 배치사이즈.\n",
        "# batch는 입력으로 받은 batch 값때마다 역전파를 진행하겠다는 의미이고,\n",
        "# batch_split은 입력으로 받은 batch_split 간격까지 gradient accumulation 하겠다는 의미.\n",
        "# 자세한건 아래 코드에서..\n",
        "mini_batch_size = args[\"batch\"] // args[\"batch_split\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDuUmD_dwXEf",
        "outputId": "bf11fc07-a589-4d57-c8ff-c1661f8d1293"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device :  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Cutout:\n",
        "    \"\"\"랜덤한 위치와 크기의 정사각형으로 이미지의 일부분을 검은색으로 변환하여\n",
        "    학습 시에 모델이 좀 더 보편적으로 이미지를 학습할 수 있게 도와주는 기법(즉 일반화를 뜻함)\"\"\"\n",
        "\n",
        "    def __init__(self, min_side=30, max_side=60, p=0.5):\n",
        "        self.max_side = max_side  # 정사각형 한변의 최대 길이\n",
        "        self.min_side = min_side  # 정사각형 한변의 최소 길이\n",
        "        self.p = p  # cutout을 진행할 확률\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # 0~1 사이의 랜덤하게 생성된 값이 self.p 이상이면 cutout 중지\n",
        "        if torch.rand([1]).item() > self.p:\n",
        "            return image\n",
        "\n",
        "        # 정사각형 한변의 길이를 min_side와 max_side 사이에서 랜덤하게 생성\n",
        "        side = torch.randint(self.min_side, self.max_side + 1, [1]).item()\n",
        "\n",
        "        # 이미지의 좌측과 위측의 좌표를 랜덤하게 구하고 (0 ~ image.size(1|2) - side),\n",
        "        # 랜덤으로 구한 정사각형 한변의 길이를 더해서 우측과 아래측의 좌표를 구한다.\n",
        "        left = torch.randint(0, image.size(1) - side, [1]).item()\n",
        "        top = torch.randint(0, image.size(2) - side, [1]).item()\n",
        "        right = left + side\n",
        "        bottom = top + side\n",
        "\n",
        "        # 구해진 좌표를 가지고 슬라이싱을 이용해 이미지의 일부분을 검은색(모든채널을 0)으로 만든다.\n",
        "        image[:, left:right, top:bottom] = 0\n",
        "        return image"
      ],
      "metadata": {
        "id": "qA3hcJtowXLQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  각 이미지에 적용할 변형기법을 파이프라인으로 정의\n",
        "#  이미지는 각 함수를 통과하여 변형됨\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        # 보간 기법으로 BILINEAR를 사용하여 160x160사이즈로 이미지를 변환시킨다.\n",
        "        # 보간 기법을 적용하면 이미지 해상도가 낮아져서 성능이 안좋을줄 알았지만,\n",
        "        # efficientnet 특성상 이미지 크기가 커야 학습 성능이 더 잘나온다.\n",
        "        transforms.Resize(160),\n",
        "        # default 값은 0.5이며, 50% 확률로 이미지 좌우를 반전시킨다.\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # 이미지를 파이토치의 텐서 객체로 변환하여 trainable하게 변환시킨다.\n",
        "        transforms.ToTensor(),\n",
        "        # 이미지를 정규화하여 학습시에 더 빠르고 안정적으로 weights를 변화시켜간다.\n",
        "        # 위에서 텐서 객체로 변환할 때 0-255범위를 0-1범위로 바꾸기 때문에\n",
        "        # ToTensor 함수 다음에 Normalize 함수를 적용시켜야한다.\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        Cutout(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 테스트 데이터에 대한 변형기법 파이프라인을 설정\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        # 보간 기법으로 BILINEAR를 사용하여 200x200사이즈로 이미지를 변환시킨다.\n",
        "        transforms.Resize(200),\n",
        "        transforms.ToTensor(),\n",
        "        # 학습때와 같이 정규화를 진행하여 같은 환경을 만들어 줌으로 써\n",
        "        # 더 정답을 잘 맞출 수 있도록 한다.\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "N-RenBvUwXAl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR100 데이터를 trainset과 testset으로 나눠서 data폴더에 다운로드 받는다.\n",
        "# 1. 폴더에 이미지데이터가 있으면 메모리에 이미지 데이터를 로드한다. 없으면 다운로드 받고 로드한다.\n",
        "# 2. transform 파이프라인을 붙여서 나중에 DataLoader가 데이터를 로드할 때\n",
        "# 각 이미지별로 하나씩 transform 파이프라인을 적용한다.\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XijMyPfdwW7H",
        "outputId": "f738dbb5-08c9-473b-e806-183d5da48caa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 97149705.58it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정답 라벨과 모델이 예측한 값의 오차를 측정하는 손실함수\n",
        "# 손실함수로 구한 오차를 토대로 나중에 옵티마이저로 weights를 업데이트 한다.\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "QfRBcQjUwWo4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x: Tensor, y, alpha=1.0, lam=1.0, count=0):\n",
        "    \"\"\"입력으로 받은 x에 해당하는 이미지들을 mixup해서 데이터 증강\n",
        "    효과를 볼 수 있게하는 함수\"\"\"\n",
        "    # count가 0이 되는 주기마다 alpha값이 존재할 시 mixup을 진행한다.\n",
        "    # count의 주기는 args.batch_split이 결정하며,\n",
        "    # 현재 코드에선 args.batch_split이 1이고, 매 번 mixup을 진행한다.\n",
        "    if count == 0:\n",
        "        if alpha > 0:\n",
        "            # 현재 args 값에 alpha는 0.1로 설정되어있어서\n",
        "            # lam의 값은 0이나 1에 가까운 값이 난수로 생성되게 된다.\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "        else:\n",
        "            # lambda가 1인 경우는 mixup을 하지 않는다.\n",
        "            # 현재 이 코드에선 alpha가 존재하기 때문에 매번 mixup한다.\n",
        "            lam = 1.0\n",
        "\n",
        "    # 매 미니배치(32)마다 이 함수가 호출되는데, 그때마다\n",
        "    # 순서가 0-31의 인덱스 이미지와 0-31범위에 해당하긴하지만\n",
        "    # 순서가 랜덤으로 생성된 이미지 ex: [1,4,6,8,0,2 ...]\n",
        "    # 를 mixup 하기위해 batch_size(32)를 인자로 랜덤순열을 생성한다.\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    # 위에서 난수로 구한 lam값과 기존 배치 0-31 인덱스와\n",
        "    # 랜덤순열로 구한 인덱스를 가지고 이미지들을 mix한다.\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # y_a는 0-31인덱스의 정답, y_b는 mixup에 사용된 이미지들의 정답\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n"
      ],
      "metadata": {
        "id": "RNStxRrrwygx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"loss function(criterion)과 모델의 예측값, mixup 라벨 2개를 받아서\n",
        "    각각 라벨당 오차를 계산하고 더한 뒤 리턴한다.\n",
        "    lam값이 1일 경우 y_a에 대한 loss값만 리턴한다.\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
      ],
      "metadata": {
        "id": "_m3gBEFnwyeb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, trainloader) -> None:\n",
        "    \"\"\"모델 학습을 담당하는 함수\"\"\"\n",
        "    # 현재 Epoch가 몇번째인지 출력한다.\n",
        "    print(\"\\nEpoch: %d\" % epoch)\n",
        "    # 모델을 학습모드로 설정한다.\n",
        "    # 학습모드에서는 gradient 계산 및 dropout이 활성화된다.\n",
        "    net.train()\n",
        "\n",
        "    # 현재 epoch에서의 가장 높은 정확도를 구한다.\n",
        "    # checkpoint 저장 시에 활용된다.\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    # 아래 4개의 지역변수들은 progress_bar 출력을 위해 사용된다.\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    count = 0\n",
        "\n",
        "    # lam 지역변수는 mixup에서 모델의 예측값이 정답인지 확인할 때 쓴다.\n",
        "    # mixup_data 함수를 거쳐나오면 lam값이 mutable하게 변하기때문에 항상 1.0이지는 않다.\n",
        "    lam = 1.0\n",
        "\n",
        "    # 학습 시작 전에 optimizer의 gradient들을 초기화 해준다.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # trainloader에서 설정한 미니배치 단위의 묶음으로 이미지와 정답라벨을 순회하며 가져온다.\n",
        "    # batch_idx는 구간마다 모델 정확도 및 loss 출력을 위해 사용된다.\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader, desc=\"Training\")):\n",
        "        # count가 batch_split과 같아지면 loss function에서 누적계산 해놓은 값을 업데이트한다.\n",
        "        # batch_split과 같아지기위해 count는 매 루프마다 1씩 증가하는 코드가 아래 존재한다.\n",
        "        if count == args[\"batch_split\"]:\n",
        "            # gradient를 업데이트 한다.\n",
        "            optimizer.step()\n",
        "            # 새 업데이트를 위해 이전 업데이트에 사용된 값들을 초기화한다.\n",
        "            optimizer.zero_grad()\n",
        "            # count를 0으로 맞춰주어서 일정 주기마다 이 if문 안으로 들어올 수 있도록 한다.\n",
        "            # 예를들어 batch_split이 2라면 짝수 주기로 gradient 업데이트를하고,\n",
        "            # 1이면 매 루프마다 업데이트한다.\n",
        "            count = 0\n",
        "        # batch_split 주기마다 gradient 업데이트 하기위해 count를 1씩 증가해준다.\n",
        "        count += 1\n",
        "        # GPU를 사용할 수 있다면 tensor 객체를 GPU에 적재하고, 그렇지 않다면 CPU에 적재한다.\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # inputs: mixup된 이미지\n",
        "        # targets_a: mixup 중 alpha에 해당하는 이미지 정답라벨\n",
        "        # targets_b: mixup 중 beta에 해당하는 이미지 정답라벨\n",
        "        # lam: 이미지 섞인 비율 값\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(\n",
        "            inputs, targets, args[\"alpha\"], lam, count\n",
        "        )\n",
        "        # efficientnet-b7에 mixup 이미지를 넣어서 predict한다.\n",
        "        outputs = net(inputs)\n",
        "        # mixup 이미지의 loss를 계산하는 함수를 이용하여 loss를 계산한다.\n",
        "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        # batch_split이 2 이상이면 gradient accumulation을 진행하니,\n",
        "        # loss를 일부분씩만 각각 미니배치에서 취해서 누적계산을 진행한다.\n",
        "        loss = loss / args[\"batch_split\"]\n",
        "        # 역전파 진행\n",
        "        loss.backward()\n",
        "\n",
        "        # 매 루프마다 train_loss를 누적증가 시킨다.\n",
        "        train_loss += loss.item()\n",
        "        # 모델이 예측한 확률 중 가장 높은 라벨만 골라서 추출한다.\n",
        "        # 1차원 벡터에 32개의 값들이 추출된다. (batch_size가 32여서)\n",
        "        _, predicted = outputs.max(1)\n",
        "        # 현 배치의 개수만큼 total에 누적합 한다.\n",
        "        total += targets.size(0)\n",
        "        # mixup 이미지를 대상으로 했기때문에 정답을 계산하는 방식도 다르다.\n",
        "        # target a와 b를 모델이 맞췄다면 lam을 활용해서 비율 조정해서 맞았다고 처리한다.\n",
        "        correct += (\n",
        "            lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "            + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float()\n",
        "        )\n",
        "\n",
        "        # accuracy : 맞은 비율 / 전체데이터 개수 * 100.0\n",
        "        acc = 100.0 * correct / total\n",
        "        epoch_acc = acc.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'[Epoch: {epoch}, Batch: {batch_idx+1}] Loss: {train_loss / (batch_idx+1):.3f}, Accuracy: {acc:.2f}%')\n",
        "\n",
        "    # 현재 에포크에서 모델이 예측한 정확도가 checkpoint threshold 이상이면\n",
        "    # checkpoint를 저장한다.\n",
        "    if epoch_acc > args[\"checkpoint_threshold\"]:\n",
        "        # 체크포인트를 저장할 경로와 파일이름을 만든다.\n",
        "        rounded_accuracy = round(epoch_acc, 2)\n",
        "        saving_ckpt_path = Path(args[\"checkpoint_save_directory\"]) / Path(\n",
        "            f'{args[\"model\"]}_{rounded_accuracy}.pt'\n",
        "        )\n",
        "\n",
        "        print(f\"Saving model : {saving_ckpt_path}\")\n",
        "\n",
        "        # 모델의 현재 가중치 상태와 정확도를 기록하여\n",
        "        # 체크포인트 저장시 입력으로 들어간다.\n",
        "        state = {\n",
        "            \"net\": net.state_dict(),\n",
        "            \"acc\": epoch_acc,\n",
        "        }\n",
        "\n",
        "        # checkpoint 폴더가 존재하지 않으면 만들어준다.\n",
        "        if not os.path.isdir(args[\"checkpoint_save_directory\"]):\n",
        "            os.mkdir(args[\"checkpoint_save_directory\"])\n",
        "\n",
        "        # 지정한 경로에 체크포인트를 실제로 세이브한다.\n",
        "        torch.save(state, saving_ckpt_path)\n"
      ],
      "metadata": {
        "id": "EKwKOpR1wycF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_files(directory, pattern):\n",
        "    \"\"\"주어진 디렉토리에서 패턴에 맞는 파일들을 찾는다.\"\"\"\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in fnmatch.filter(files, pattern):\n",
        "            yield os.path.join(root, filename)\n"
      ],
      "metadata": {
        "id": "dJChPky_OKM1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(testloader) -> None:\n",
        "    \"\"\"현재 환경 변수에 저장되어있는 모델 이름의 체크포인트를\n",
        "        전부 읽어 들여서 test를 진행한다.\"\"\"\n",
        "    # 모델을 평가모드로 전환해서 dropout 및 gredient 변동이 일어나지 않게한다.\n",
        "    net.eval()\n",
        "\n",
        "    # checkpoint들이 있는 directory에서 현재 model의 이름이 들어간\n",
        "    # 모든 checkpoint를 순회하면서 정확도를 전부 측정한다.\n",
        "    for checkpoint_path in find_files(\n",
        "        args[\"checkpoint_save_directory\"], f'*{args[\"model\"]}*'\n",
        "    ):\n",
        "        print(\"Loading checkpoint..\")\n",
        "        # 체크포인트를 load한다.\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        # load한 체크포인트를 모델에 적용한다.\n",
        "        net.load_state_dict(checkpoint[\"net\"])\n",
        "\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        print(f\"{'-' * 10} 모델 테스트 시작 {'-' * 10}\")\n",
        "        # gradient를 계산하지 않는 상태에서 모델의 정확도를 측정한다.\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(tqdm(testloader)):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # 모델 정확도 측정 및 프린트\n",
        "        acc = 100.0 * correct / total\n",
        "        print(\n",
        "            f\"File_path : { checkpoint_path }, Test Loss: {test_loss / len(testloader):.3f}, Test Accuracy: {acc:.2f}%\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "wGzjCecNwyZv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더를 이용해서 배치사이즈 크기별로 iterate 할 수 있도록 한다.\n",
        "# 테스트는 shuffle을 하든 안하든 상관없기 때문에 성능상의 이유로 False이고,\n",
        "# 트레인은 shuffle을 해야 매 에포크 및 미니배치마다 다양한 조합의 이미지들이\n",
        "# 배치 정규화 및 mixup 되기때문에 하면 일반화 및 모델 성능에 좋다.\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=10, shuffle=False, num_workers=1\n",
        ")\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=mini_batch_size, shuffle=True, num_workers=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pfeg4f3XwyXJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델은 EfficientNet을 이용한다.\n",
        "net = EfficientNet.from_pretrained(args[\"model\"], num_classes=100)\n",
        "net = net.to(device)\n",
        "optimizer = optim.SGD(\n",
        "    net.parameters(), lr=args[\"lr\"], momentum=0.9, weight_decay=args[\"wd\"]\n",
        ")\n",
        "# StepLR을 이용해서 에포크가 진행될수록 lr를 점점 줄이도록 한다.\n",
        "lr_sc = lr_scheduler.StepLR(optimizer, step_size=args[\"nsc\"], gamma=args[\"gamma\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYrKnI4WwyPV",
        "outputId": "0ccac033-e256-4d92-eba4-2c217e11b622"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n",
            "100%|██████████| 254M/254M [00:01<00:00, 161MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training을 하기 위한 for loop\n",
        "for epoch in range(0, args[\"ne\"]):\n",
        "    train(epoch, trainloader)\n",
        "    # learning rate를 줄인다.\n",
        "    lr_sc.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "U9bQ5oLuxHP4",
        "outputId": "eb7c6500-7da1-4ec4-bd64-78bd75fbecbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/1563 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Training:   6%|▋         | 100/1563 [01:18<19:09,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 100] Loss: 4.346, Accuracy: 11.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 200/1563 [02:38<17:54,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 200] Loss: 3.638, Accuracy: 25.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|█▉        | 300/1563 [03:58<16:49,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 300] Loss: 2.989, Accuracy: 35.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|██▌       | 400/1563 [05:17<15:24,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 400] Loss: 2.568, Accuracy: 43.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      | 500/1563 [06:37<14:05,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 500] Loss: 2.284, Accuracy: 48.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 600/1563 [07:56<13:05,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 600] Loss: 2.088, Accuracy: 51.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  45%|████▍     | 700/1563 [09:16<11:30,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 700] Loss: 1.944, Accuracy: 54.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 800/1563 [10:35<10:04,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 800] Loss: 1.829, Accuracy: 56.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 900/1563 [11:55<08:45,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 900] Loss: 1.739, Accuracy: 57.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   | 1000/1563 [13:15<07:31,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1000] Loss: 1.659, Accuracy: 59.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|███████   | 1100/1563 [14:34<06:07,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1100] Loss: 1.594, Accuracy: 60.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  77%|███████▋  | 1200/1563 [15:54<04:46,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1200] Loss: 1.534, Accuracy: 61.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  83%|████████▎ | 1300/1563 [17:13<03:27,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1300] Loss: 1.482, Accuracy: 62.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  90%|████████▉ | 1400/1563 [18:33<02:10,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1400] Loss: 1.436, Accuracy: 63.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 1500/1563 [19:52<00:49,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 0, Batch: 1500] Loss: 1.391, Accuracy: 64.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  98%|█████████▊| 1532/1563 [20:19<00:24,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ed6c4aa55c5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training을 하기 위한 for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ne\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# learning rate를 줄인다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1828ada79bd0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, trainloader)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# 역전파 진행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# 매 루프마다 train_loss를 누적증가 시킨다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(testloader)\n"
      ],
      "metadata": {
        "id": "sW7zHufaxNA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}